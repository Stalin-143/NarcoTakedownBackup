{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading data from: data\n",
      "Loaded 110 samples\n",
      "Building models...\n",
      "Building text model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ec80390f8e437cb884f4a3e327a89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b30c85b1784301b522e01ca3c0c7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57be1fd570cc40e89f9e5329bbfff393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb5a6ad823f4e65a38204b0b23fe340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffe969410864fef95085b11ac7b88a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text model performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Building image model...\n",
      "Image model would be implemented here with your specific image data\n",
      "Building combined model...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get local object 'NarcoticWebsiteClassifier._build_image_model.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 327\u001b[39m\n\u001b[32m    324\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 307\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    304\u001b[39m classifier.build_models()\n\u001b[32m    306\u001b[39m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# Test the classifier on some example URLs\u001b[39;00m\n\u001b[32m    310\u001b[39m test_urls = [\n\u001b[32m    311\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhttps://example.com\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    312\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msomefakesite.onion\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    313\u001b[39m ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 274\u001b[39m, in \u001b[36mNarcoticWebsiteClassifier.save_model\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo model to save. Train the model first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     \u001b[43mpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcombined_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcombined_model\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: Can't get local object 'NarcoticWebsiteClassifier._build_image_model.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class NarcoticWebsiteClassifier:\n",
    "    def __init__(self, data_dir=\"data\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.text_model = None\n",
    "        self.image_model = None\n",
    "        self.combined_model = None\n",
    "        self.tokenizer = None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess all data from the data directory\"\"\"\n",
    "        print(\"Loading data from:\", self.data_dir)\n",
    "        self.data = {\n",
    "            \"text\": [],\n",
    "            \"images\": [],\n",
    "            \"urls\": [],\n",
    "            \"labels\": []\n",
    "        }\n",
    "        \n",
    "        # Recursively walk through all subdirectories\n",
    "        for root, dirs, files in os.walk(self.data_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_ext = os.path.splitext(file)[1].lower()\n",
    "                \n",
    "                # Process based on file type\n",
    "                if file_ext in ['.txt', '.csv', '.json']:\n",
    "                    self._process_text_file(file_path, file_ext)\n",
    "                elif file_ext in ['.jpg', '.jpeg', '.png']:\n",
    "                    self._process_image_file(file_path)\n",
    "        \n",
    "        print(f\"Loaded {len(self.data['labels'])} samples\")\n",
    "        return self.data\n",
    "    \n",
    "    def _process_text_file(self, file_path, file_ext):\n",
    "        \"\"\"Process text files based on their extension\"\"\"\n",
    "        try:\n",
    "            if file_ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    # Assuming each text file has content and a label (narcotic or not)\n",
    "                    # You'll need to adapt this based on your actual data structure\n",
    "                    is_narcotic = self._check_narcotic_keywords(content)\n",
    "                    self.data[\"text\"].append(content)\n",
    "                    self.data[\"labels\"].append(is_narcotic)\n",
    "                    self.data[\"urls\"].append(self._extract_url(content))\n",
    "                    self.data[\"images\"].append(None)\n",
    "                    \n",
    "            elif file_ext == '.csv':\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Adjust column names based on your CSV structure\n",
    "                if all(col in df.columns for col in ['content', 'url', 'is_narcotic']):\n",
    "                    for _, row in df.iterrows():\n",
    "                        self.data[\"text\"].append(row['content'])\n",
    "                        self.data[\"urls\"].append(row['url'])\n",
    "                        self.data[\"labels\"].append(row['is_narcotic'])\n",
    "                        self.data[\"images\"].append(None)\n",
    "                \n",
    "            elif file_ext == '.json':\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    json_data = json.load(f)\n",
    "                    # Process JSON based on your structure\n",
    "                    if isinstance(json_data, list):\n",
    "                        for item in json_data:\n",
    "                            if all(key in item for key in ['content', 'url', 'is_narcotic']):\n",
    "                                self.data[\"text\"].append(item['content'])\n",
    "                                self.data[\"urls\"].append(item['url'])\n",
    "                                self.data[\"labels\"].append(item['is_narcotic'])\n",
    "                                self.data[\"images\"].append(None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    def _process_image_file(self, file_path):\n",
    "        \"\"\"Process image files\"\"\"\n",
    "        try:\n",
    "            # For images, we'll need labels from a separate source or from the file path\n",
    "            # This is a placeholder - adapt to your data organization\n",
    "            parent_dir = os.path.basename(os.path.dirname(file_path))\n",
    "            is_narcotic = 'narcotic' in parent_dir.lower()\n",
    "            \n",
    "            self.data[\"images\"].append(file_path)\n",
    "            self.data[\"text\"].append(None)\n",
    "            self.data[\"urls\"].append(None)\n",
    "            self.data[\"labels\"].append(is_narcotic)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {file_path}: {e}\")\n",
    "    \n",
    "    def _check_narcotic_keywords(self, text):\n",
    "        \"\"\"Simple keyword check - replace with your actual logic\"\"\"\n",
    "        keywords = ['narcotic', 'drug', 'cocaine', 'heroin', 'mdma', 'lsd', \n",
    "                   'marijuana', 'cannabis', 'buy drugs', 'pills', 'opioid']\n",
    "        return any(keyword in text.lower() for keyword in keywords)\n",
    "    \n",
    "    def _extract_url(self, text):\n",
    "        \"\"\"Extract URL from text if present\"\"\"\n",
    "        url_pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+|(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\\.onion'\n",
    "        match = re.search(url_pattern, text)\n",
    "        return match.group(0) if match else None\n",
    "    \n",
    "    def build_models(self):\n",
    "        \"\"\"Build and train the models\"\"\"\n",
    "        print(\"Building models...\")\n",
    "        # 1. Text model using BERT\n",
    "        self._build_text_model()\n",
    "        \n",
    "        # 2. Image model\n",
    "        self._build_image_model()\n",
    "        \n",
    "        # 3. Combined model\n",
    "        self._build_combined_model()\n",
    "        \n",
    "    def _build_text_model(self):\n",
    "        \"\"\"Build and train the text classification model\"\"\"\n",
    "        print(\"Building text model...\")\n",
    "        # Filter data to include only text samples\n",
    "        text_data = [(text, label) for text, label in zip(self.data[\"text\"], self.data[\"labels\"]) if text is not None]\n",
    "        \n",
    "        if not text_data:\n",
    "            print(\"No text data available to train text model\")\n",
    "            return\n",
    "            \n",
    "        texts, labels = zip(*text_data)\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        text_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Extract features\n",
    "        features = []\n",
    "        for text in texts:\n",
    "            inputs = self.tokenizer(text, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = text_model(**inputs)\n",
    "            # Use CLS token as feature vector\n",
    "            features.append(outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten())\n",
    "        \n",
    "        # Train a classifier\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "        \n",
    "        self.text_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.text_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = self.text_model.predict(X_test)\n",
    "        print(\"Text model performance:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    def _build_image_model(self):\n",
    "        \"\"\"Build and train the image classification model\"\"\"\n",
    "        print(\"Building image model...\")\n",
    "        # Filter data to include only image samples\n",
    "        image_data = [(img, label) for img, label in zip(self.data[\"images\"], self.data[\"labels\"]) if img is not None]\n",
    "        \n",
    "        if not image_data:\n",
    "            print(\"No image data available to train image model\")\n",
    "            return\n",
    "            \n",
    "        # Here we would implement a CNN or use a pre-trained model\n",
    "        # This is a placeholder since actual implementation would depend on your specific requirements\n",
    "        print(\"Image model would be implemented here with your specific image data\")\n",
    "        \n",
    "        # For now, we'll create a simple model that always returns False\n",
    "        self.image_model = lambda x: False\n",
    "    \n",
    "    def _build_combined_model(self):\n",
    "        \"\"\"Build a model that combines text and image features\"\"\"\n",
    "        print(\"Building combined model...\")\n",
    "        # This would combine the outputs of the text and image models\n",
    "        # Placeholder for actual implementation\n",
    "        self.combined_model = self.text_model  # For now, just use the text model\n",
    "    \n",
    "    def classify_website(self, url):\n",
    "        \"\"\"Classify a website as narcotic or not\"\"\"\n",
    "        print(f\"Analyzing website: {url}\")\n",
    "        \n",
    "        if self.text_model is None:\n",
    "            raise ValueError(\"Model has not been trained. Call build_models() first.\")\n",
    "        \n",
    "        # Check if it's an onion URL\n",
    "        is_onion = '.onion' in url\n",
    "        \n",
    "        try:\n",
    "            # For .onion URLs, we would need a Tor setup\n",
    "            if is_onion:\n",
    "                print(\"Onion URL detected. Using pre-configured proxy for Tor access...\")\n",
    "                # This is where you'd implement Tor proxy access\n",
    "                # For now, we'll use features that suggest it's likely narcotic\n",
    "                content = \"This is a placeholder for Tor hidden service content\"\n",
    "            else:\n",
    "                # For regular URLs, fetch the content\n",
    "                response = requests.get(url, timeout=10)\n",
    "                content = response.text\n",
    "                \n",
    "            # Extract text features\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            text_content = soup.get_text()\n",
    "            \n",
    "            # Tokenize and get features\n",
    "            inputs = self.tokenizer(text_content, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                text_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "                outputs = text_model(**inputs)\n",
    "            \n",
    "            # Use CLS token as feature vector\n",
    "            features = outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten().reshape(1, -1)\n",
    "            \n",
    "            # Make prediction\n",
    "            prediction = self.text_model.predict(features)[0]\n",
    "            probability = self.text_model.predict_proba(features)[0][1]  # Probability of being narcotic\n",
    "            \n",
    "            # Additional signals that might indicate a narcotic website\n",
    "            additional_signals = {\n",
    "                \"is_onion\": is_onion,\n",
    "                \"keyword_match\": self._check_narcotic_keywords(text_content),\n",
    "                \"suspicious_patterns\": self._check_suspicious_patterns(text_content, url)\n",
    "            }\n",
    "            \n",
    "            # Final decision considering all signals\n",
    "            is_narcotic = prediction or (additional_signals[\"is_onion\"] and additional_signals[\"keyword_match\"])\n",
    "            \n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"is_narcotic\": is_narcotic,\n",
    "                \"confidence\": probability,\n",
    "                \"additional_signals\": additional_signals\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {url}: {e}\")\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"is_narcotic\": None,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _check_suspicious_patterns(self, content, url):\n",
    "        \"\"\"Check for patterns common in narcotic websites\"\"\"\n",
    "        patterns = [\n",
    "            r'bitcoin|btc|monero|xmr|cryptocurrency',  # Payment methods\n",
    "            r'escrow|vendor|marketplace',              # Marketplace terminology\n",
    "            r'anonymous|encrypted|secure',             # Security terms\n",
    "            r'shipping|delivery|tracking',             # Shipping terms\n",
    "            r'telegram|wickr|signal'                   # Communication apps\n",
    "        ]\n",
    "        \n",
    "        return any(re.search(pattern, content.lower()) for pattern in patterns)\n",
    "    \n",
    "    def save_model(self, path=\"narcotic_classifier.pkl\"):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        if self.text_model is None:\n",
    "            raise ValueError(\"No model to save. Train the model first.\")\n",
    "            \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'text_model': self.text_model,\n",
    "                'image_model': self.image_model,\n",
    "                'combined_model': self.combined_model\n",
    "            }, f)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    def load_model(self, path=\"narcotic_classifier.pkl\"):\n",
    "        \"\"\"Load a saved model\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            models = pickle.load(f)\n",
    "            self.text_model = models['text_model']\n",
    "            self.image_model = models['image_model']\n",
    "            self.combined_model = models['combined_model']\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        \n",
    "        # Load the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        print(\"Model loaded successfully\")\n",
    "        \n",
    "# Example usage\n",
    "def main():\n",
    "    # Initialize the classifier\n",
    "    classifier = NarcoticWebsiteClassifier(data_dir=\"data\")\n",
    "    \n",
    "    # Load and preprocess the data\n",
    "    classifier.load_data()\n",
    "    \n",
    "    # Build and train the models\n",
    "    classifier.build_models()\n",
    "    \n",
    "    # Save the model\n",
    "    classifier.save_model()\n",
    "    \n",
    "    # Test the classifier on some example URLs\n",
    "    test_urls = [\n",
    "        \"https://example.com\",\n",
    "        \"somefakesite.onion\"\n",
    "    ]\n",
    "    \n",
    "    for url in test_urls:\n",
    "        result = classifier.classify_website(url)\n",
    "        print(f\"\\nResults for {url}:\")\n",
    "        print(f\"  Is narcotic: {result['is_narcotic']}\")\n",
    "        if 'confidence' in result:\n",
    "            print(f\"  Confidence: {result['confidence']:.2f}\")\n",
    "        if 'additional_signals' in result:\n",
    "            print(f\"  Additional signals: {result['additional_signals']}\")\n",
    "        if 'error' in result:\n",
    "            print(f\"  Error: {result['error']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyImageModel:\n",
    "    \"\"\"A simple class to replace the lambda function for image classification\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Always predict non-narcotic for now\n",
    "        return [False] * len(X) if hasattr(X, '__len__') else False\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        # Return probabilities of [not narcotic, narcotic]\n",
    "        return [[0.9, 0.1]] * len(X) if hasattr(X, '__len__') else [0.9, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Find this line in your code:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m.image_model = \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Replace it with:\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mself\u001b[39m.image_model = DummyImageModel()\n",
      "\u001b[31mNameError\u001b[39m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# Find this line in your code:\n",
    "self.image_model = lambda x: False\n",
    "\n",
    "# Replace it with:\n",
    "self.image_model = DummyImageModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading data from: data\n",
      "Loaded 110 samples\n",
      "Building models...\n",
      "Building text model...\n",
      "Text model performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Building image model...\n",
      "Image model would be implemented here with your specific image data\n",
      "Building combined model...\n",
      "Error running main: Can't get local object 'NarcoticWebsiteClassifier._build_image_model.<locals>.<lambda>'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"Error running main: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading data from: data\n",
      "Loaded 110 samples\n",
      "Building models...\n",
      "Building text model...\n",
      "Text model performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Building image model...\n",
      "Image model would be implemented here with your specific image data\n",
      "Building combined model...\n",
      "Model saved to narcotic_classifier.pkl\n",
      "Analyzing website: https://example.com\n",
      "\n",
      "Results for https://example.com:\n",
      "  Is narcotic: True\n",
      "  Confidence: 0.74\n",
      "  Additional signals: {'is_onion': False, 'keyword_match': False, 'suspicious_patterns': False}\n",
      "Analyzing website: somefakesite.onion\n",
      "Onion URL detected. Using pre-configured proxy for Tor access...\n",
      "\n",
      "Results for somefakesite.onion:\n",
      "  Is narcotic: True\n",
      "  Confidence: 0.78\n",
      "  Additional signals: {'is_onion': True, 'keyword_match': False, 'suspicious_patterns': False}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class DummyImageModel:\n",
    "    \"\"\"A simple class to replace the lambda function for image classification\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Always predict non-narcotic for now\n",
    "        return [False] * len(X) if hasattr(X, '__len__') else False\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        # Return probabilities of [not narcotic, narcotic]\n",
    "        return [[0.9, 0.1]] * len(X) if hasattr(X, '__len__') else [0.9, 0.1]\n",
    "\n",
    "class NarcoticWebsiteClassifier:\n",
    "    def __init__(self, data_dir=\"data\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.text_model = None\n",
    "        self.image_model = None\n",
    "        self.combined_model = None\n",
    "        self.tokenizer = None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess all data from the data directory\"\"\"\n",
    "        print(\"Loading data from:\", self.data_dir)\n",
    "        self.data = {\n",
    "            \"text\": [],\n",
    "            \"images\": [],\n",
    "            \"urls\": [],\n",
    "            \"labels\": []\n",
    "        }\n",
    "        \n",
    "        # Recursively walk through all subdirectories\n",
    "        for root, dirs, files in os.walk(self.data_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_ext = os.path.splitext(file)[1].lower()\n",
    "                \n",
    "                # Process based on file type\n",
    "                if file_ext in ['.txt', '.csv', '.json']:\n",
    "                    self._process_text_file(file_path, file_ext)\n",
    "                elif file_ext in ['.jpg', '.jpeg', '.png']:\n",
    "                    self._process_image_file(file_path)\n",
    "        \n",
    "        print(f\"Loaded {len(self.data['labels'])} samples\")\n",
    "        return self.data\n",
    "    \n",
    "    def _process_text_file(self, file_path, file_ext):\n",
    "        \"\"\"Process text files based on their extension\"\"\"\n",
    "        try:\n",
    "            if file_ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    # Assuming each text file has content and a label (narcotic or not)\n",
    "                    # You'll need to adapt this based on your actual data structure\n",
    "                    is_narcotic = self._check_narcotic_keywords(content)\n",
    "                    self.data[\"text\"].append(content)\n",
    "                    self.data[\"labels\"].append(is_narcotic)\n",
    "                    self.data[\"urls\"].append(self._extract_url(content))\n",
    "                    self.data[\"images\"].append(None)\n",
    "                    \n",
    "            elif file_ext == '.csv':\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Adjust column names based on your CSV structure\n",
    "                if all(col in df.columns for col in ['content', 'url', 'is_narcotic']):\n",
    "                    for _, row in df.iterrows():\n",
    "                        self.data[\"text\"].append(row['content'])\n",
    "                        self.data[\"urls\"].append(row['url'])\n",
    "                        self.data[\"labels\"].append(row['is_narcotic'])\n",
    "                        self.data[\"images\"].append(None)\n",
    "                \n",
    "            elif file_ext == '.json':\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    json_data = json.load(f)\n",
    "                    # Process JSON based on your structure\n",
    "                    if isinstance(json_data, list):\n",
    "                        for item in json_data:\n",
    "                            if all(key in item for key in ['content', 'url', 'is_narcotic']):\n",
    "                                self.data[\"text\"].append(item['content'])\n",
    "                                self.data[\"urls\"].append(item['url'])\n",
    "                                self.data[\"labels\"].append(item['is_narcotic'])\n",
    "                                self.data[\"images\"].append(None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    def _process_image_file(self, file_path):\n",
    "        \"\"\"Process image files\"\"\"\n",
    "        try:\n",
    "            # For images, we'll need labels from a separate source or from the file path\n",
    "            # This is a placeholder - adapt to your data organization\n",
    "            parent_dir = os.path.basename(os.path.dirname(file_path))\n",
    "            is_narcotic = 'narcotic' in parent_dir.lower()\n",
    "            \n",
    "            self.data[\"images\"].append(file_path)\n",
    "            self.data[\"text\"].append(None)\n",
    "            self.data[\"urls\"].append(None)\n",
    "            self.data[\"labels\"].append(is_narcotic)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {file_path}: {e}\")\n",
    "    \n",
    "    def _check_narcotic_keywords(self, text):\n",
    "        \"\"\"Simple keyword check - replace with your actual logic\"\"\"\n",
    "        keywords = ['narcotic', 'drug', 'cocaine', 'heroin', 'mdma', 'lsd', \n",
    "                   'marijuana', 'cannabis', 'buy drugs', 'pills', 'opioid']\n",
    "        return any(keyword in text.lower() for keyword in keywords)\n",
    "    \n",
    "    def _extract_url(self, text):\n",
    "        \"\"\"Extract URL from text if present\"\"\"\n",
    "        url_pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+|(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\\.onion'\n",
    "        match = re.search(url_pattern, text)\n",
    "        return match.group(0) if match else None\n",
    "    \n",
    "    def build_models(self):\n",
    "        \"\"\"Build and train the models\"\"\"\n",
    "        print(\"Building models...\")\n",
    "        # 1. Text model using BERT\n",
    "        self._build_text_model()\n",
    "        \n",
    "        # 2. Image model\n",
    "        self._build_image_model()\n",
    "        \n",
    "        # 3. Combined model\n",
    "        self._build_combined_model()\n",
    "        \n",
    "    def _build_text_model(self):\n",
    "        \"\"\"Build and train the text classification model\"\"\"\n",
    "        print(\"Building text model...\")\n",
    "        # Filter data to include only text samples\n",
    "        text_data = [(text, label) for text, label in zip(self.data[\"text\"], self.data[\"labels\"]) if text is not None]\n",
    "        \n",
    "        if not text_data:\n",
    "            print(\"No text data available to train text model\")\n",
    "            return\n",
    "            \n",
    "        texts, labels = zip(*text_data)\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        text_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Extract features\n",
    "        features = []\n",
    "        for text in texts:\n",
    "            inputs = self.tokenizer(text, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = text_model(**inputs)\n",
    "            # Use CLS token as feature vector\n",
    "            features.append(outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten())\n",
    "        \n",
    "        # Train a classifier\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "        \n",
    "        self.text_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.text_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = self.text_model.predict(X_test)\n",
    "        print(\"Text model performance:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    def _build_image_model(self):\n",
    "        \"\"\"Build and train the image classification model\"\"\"\n",
    "        print(\"Building image model...\")\n",
    "        # Filter data to include only image samples\n",
    "        image_data = [(img, label) for img, label in zip(self.data[\"images\"], self.data[\"labels\"]) if img is not None]\n",
    "        \n",
    "        if not image_data:\n",
    "            print(\"No image data available to train image model\")\n",
    "            return\n",
    "            \n",
    "        # Here we would implement a CNN or use a pre-trained model\n",
    "        # This is a placeholder since actual implementation would depend on your specific requirements\n",
    "        print(\"Image model would be implemented here with your specific image data\")\n",
    "        \n",
    "        # Create a properly picklable dummy model instead of using lambda\n",
    "        self.image_model = DummyImageModel()\n",
    "    \n",
    "    def _build_combined_model(self):\n",
    "        \"\"\"Build a model that combines text and image features\"\"\"\n",
    "        print(\"Building combined model...\")\n",
    "        # This would combine the outputs of the text and image models\n",
    "        # For now, just use the text model\n",
    "        self.combined_model = self.text_model\n",
    "    \n",
    "    def classify_website(self, url):\n",
    "        \"\"\"Classify a website as narcotic or not\"\"\"\n",
    "        print(f\"Analyzing website: {url}\")\n",
    "        \n",
    "        if self.text_model is None:\n",
    "            raise ValueError(\"Model has not been trained. Call build_models() first.\")\n",
    "        \n",
    "        # Check if it's an onion URL\n",
    "        is_onion = '.onion' in url\n",
    "        \n",
    "        try:\n",
    "            # For .onion URLs, we would need a Tor setup\n",
    "            if is_onion:\n",
    "                print(\"Onion URL detected. Using pre-configured proxy for Tor access...\")\n",
    "                # This is where you'd implement Tor proxy access\n",
    "                # For now, we'll use features that suggest it's likely narcotic\n",
    "                content = \"This is a placeholder for Tor hidden service content\"\n",
    "            else:\n",
    "                # For regular URLs, fetch the content\n",
    "                response = requests.get(url, timeout=10)\n",
    "                content = response.text\n",
    "                \n",
    "            # Extract text features\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            text_content = soup.get_text()\n",
    "            \n",
    "            # Tokenize and get features\n",
    "            inputs = self.tokenizer(text_content, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                text_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "                outputs = text_model(**inputs)\n",
    "            \n",
    "            # Use CLS token as feature vector\n",
    "            features = outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten().reshape(1, -1)\n",
    "            \n",
    "            # Make prediction\n",
    "            prediction = self.text_model.predict(features)[0]\n",
    "            probability = self.text_model.predict_proba(features)[0][1]  # Probability of being narcotic\n",
    "            \n",
    "            # Additional signals that might indicate a narcotic website\n",
    "            additional_signals = {\n",
    "                \"is_onion\": is_onion,\n",
    "                \"keyword_match\": self._check_narcotic_keywords(text_content),\n",
    "                \"suspicious_patterns\": self._check_suspicious_patterns(text_content, url)\n",
    "            }\n",
    "            \n",
    "            # Final decision considering all signals\n",
    "            is_narcotic = prediction or (additional_signals[\"is_onion\"] and additional_signals[\"keyword_match\"])\n",
    "            \n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"is_narcotic\": is_narcotic,\n",
    "                \"confidence\": probability,\n",
    "                \"additional_signals\": additional_signals\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {url}: {e}\")\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"is_narcotic\": None,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _check_suspicious_patterns(self, content, url):\n",
    "        \"\"\"Check for patterns common in narcotic websites\"\"\"\n",
    "        patterns = [\n",
    "            r'bitcoin|btc|monero|xmr|cryptocurrency',  # Payment methods\n",
    "            r'escrow|vendor|marketplace',              # Marketplace terminology\n",
    "            r'anonymous|encrypted|secure',             # Security terms\n",
    "            r'shipping|delivery|tracking',             # Shipping terms\n",
    "            r'telegram|wickr|signal'                   # Communication apps\n",
    "        ]\n",
    "        \n",
    "        return any(re.search(pattern, content.lower()) for pattern in patterns)\n",
    "    \n",
    "    def save_model(self, path=\"narcotic_classifier.pkl\"):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        if self.text_model is None:\n",
    "            raise ValueError(\"No model to save. Train the model first.\")\n",
    "            \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'text_model': self.text_model,\n",
    "                'image_model': self.image_model,\n",
    "                'combined_model': self.combined_model\n",
    "            }, f)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    def load_model(self, path=\"narcotic_classifier.pkl\"):\n",
    "        \"\"\"Load a saved model\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            models = pickle.load(f)\n",
    "            self.text_model = models['text_model']\n",
    "            self.image_model = models['image_model']\n",
    "            self.combined_model = models['combined_model']\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        \n",
    "        # Load the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        print(\"Model loaded successfully\")\n",
    "        \n",
    "# Example usage\n",
    "def main():\n",
    "    # Initialize the classifier\n",
    "    classifier = NarcoticWebsiteClassifier(data_dir=\"data\")\n",
    "    \n",
    "    # Load and preprocess the data\n",
    "    classifier.load_data()\n",
    "    \n",
    "    # Build and train the models\n",
    "    classifier.build_models()\n",
    "    \n",
    "    try:\n",
    "        # Save the model\n",
    "        classifier.save_model()\n",
    "        \n",
    "        # Test the classifier on some example URLs\n",
    "        test_urls = [\n",
    "            \"https://example.com\",\n",
    "            \"somefakesite.onion\"\n",
    "        ]\n",
    "        \n",
    "        for url in test_urls:\n",
    "            result = classifier.classify_website(url)\n",
    "            print(f\"\\nResults for {url}:\")\n",
    "            print(f\"  Is narcotic: {result['is_narcotic']}\")\n",
    "            if 'confidence' in result:\n",
    "                print(f\"  Confidence: {result['confidence']:.2f}\")\n",
    "            if 'additional_signals' in result:\n",
    "                print(f\"  Additional signals: {result['additional_signals']}\")\n",
    "            if 'error' in result:\n",
    "                print(f\"  Error: {result['error']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_url(url):\n",
    "    \"\"\"Test a specific URL with the trained classifier\"\"\"\n",
    "    try:\n",
    "        # Load the trained classifier\n",
    "        classifier = NarcoticWebsiteClassifier()\n",
    "        classifier.load_model(\"narcotic_classifier.pkl\")\n",
    "        \n",
    "        # Analyze the URL\n",
    "        print(f\"Analyzing: {url}\")\n",
    "        result = classifier.classify_website(url)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\nResults:\")\n",
    "        print(f\"  Is narcotic: {result['is_narcotic']}\")\n",
    "        if 'confidence' in result:\n",
    "            print(f\"  Confidence: {result['confidence']:.2f}\")\n",
    "        if 'additional_signals' in result:\n",
    "            print(f\"  Additional signals:\")\n",
    "            for key, value in result['additional_signals'].items():\n",
    "                print(f\"    - {key}: {value}\")\n",
    "        if 'error' in result:\n",
    "            print(f\"  Error: {result['error']}\")\n",
    "            \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing URL: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded from narcotic_classifier.pkl\n",
      "Model loaded successfully\n",
      "Analyzing: http://vbfdaifjmi2oauuvwuyjdwgyoo665psqix37odwd6j75vhqfe7rdk2ad.onion/product/buy-nembutal-suicide-drug/\n",
      "Analyzing website: http://vbfdaifjmi2oauuvwuyjdwgyoo665psqix37odwd6j75vhqfe7rdk2ad.onion/product/buy-nembutal-suicide-drug/\n",
      "Onion URL detected. Using pre-configured proxy for Tor access...\n",
      "\n",
      "Results:\n",
      "  Is narcotic: True\n",
      "  Confidence: 0.78\n",
      "  Additional signals:\n",
      "    - is_onion: True\n",
      "    - keyword_match: False\n",
      "    - suspicious_patterns: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'url': 'http://vbfdaifjmi2oauuvwuyjdwgyoo665psqix37odwd6j75vhqfe7rdk2ad.onion/product/buy-nembutal-suicide-drug/',\n",
       " 'is_narcotic': np.True_,\n",
       " 'confidence': np.float64(0.78),\n",
       " 'additional_signals': {'is_onion': True,\n",
       "  'keyword_match': False,\n",
       "  'suspicious_patterns': False}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_url(\"http://vbfdaifjmi2oauuvwuyjdwgyoo665psqix37odwd6j75vhqfe7rdk2ad.onion/product/buy-nembutal-suicide-drug/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e6a76695d54a52b9d1f8ac5f34ce4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Narcotic Website Analyzer'), Text(value='https://example.com', description='URL:',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create input widget\n",
    "url_input = widgets.Text(\n",
    "    value='https://example.com',\n",
    "    placeholder='Enter URL to check',\n",
    "    description='URL:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "# Create button\n",
    "check_button = widgets.Button(\n",
    "    description='Check Website',\n",
    "    disabled=False,\n",
    "    button_style='primary',\n",
    "    tooltip='Click to analyze the URL'\n",
    ")\n",
    "\n",
    "# Create output area\n",
    "output = widgets.Output()\n",
    "\n",
    "# Button click handler\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        url = url_input.value\n",
    "        if url:\n",
    "            test_url(url)\n",
    "        else:\n",
    "            print(\"Please enter a URL\")\n",
    "\n",
    "# Connect button to function\n",
    "check_button.on_click(on_button_clicked)\n",
    "\n",
    "# Display the UI\n",
    "display(widgets.VBox([widgets.Label(\"Narcotic Website Analyzer\"), url_input, check_button, output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image classification model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image model loaded\n",
      "Using device: cpu\n",
      "Model loaded from narcotic_classifier.pkl\n",
      "Loading image classification model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image model loaded\n",
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "classifier = NarcoticWebsiteClassifier()\n",
    "classifier.load_model(\"narcotic_classifier.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e98317e26045e1bf26080895cb509e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Narcotic Website Analyzer'), Text(value='https://example.com', description='URL:',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create input widget\n",
    "url_input = widgets.Text(\n",
    "    value='https://example.com',\n",
    "    placeholder='Enter URL to check',\n",
    "    description='URL:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "# Create button\n",
    "check_button = widgets.Button(\n",
    "    description='Check Website',\n",
    "    disabled=False,\n",
    "    button_style='primary',\n",
    "    tooltip='Click to analyze the URL'\n",
    ")\n",
    "\n",
    "# Create output area\n",
    "output = widgets.Output()\n",
    "\n",
    "# Button click handler\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        url = url_input.value\n",
    "        if url:\n",
    "            test_url(url)\n",
    "        else:\n",
    "            print(\"Please enter a URL\")\n",
    "\n",
    "# Connect button to function\n",
    "check_button.on_click(on_button_clicked)\n",
    "\n",
    "# Display the UI\n",
    "display(widgets.VBox([widgets.Label(\"Narcotic Website Analyzer\"), url_input, check_button, output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Updated model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "\n",
    "def load_model(model_file=\"narcotic_classifier.pkl\"):\n",
    "    \"\"\"Loads the latest trained model.\"\"\"\n",
    "    if not os.path.exists(model_file):\n",
    "        print(\"âš ï¸ Model file not found! Train the model first.\")\n",
    "        return None\n",
    "    \n",
    "    model = load(model_file)\n",
    "    print(\"âœ… Updated model loaded successfully!\")\n",
    "    return model\n",
    "\n",
    "# Load the updated model\n",
    "classifier = load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6defd3e968c641c08523879a24cfdfff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Narcotic Website Analyzer'), Text(value='http://moneybahbkzqner2efp4tg2ev73mxt5amkâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(widgets.VBox([widgets.Label(\"Narcotic Website Analyzer\"), url_input, check_button, output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyImageModel:\n",
    "    \"\"\"A simple class to replace the lambda function for image classification\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Always predict non-narcotic for now\n",
    "        if isinstance(X, list) or hasattr(X, 'shape'):\n",
    "            return [False] * (len(X) if isinstance(X, list) else X.shape[0])\n",
    "        return False\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        # Return probabilities of [not narcotic, narcotic]\n",
    "        if isinstance(X, list) or hasattr(X, 'shape'):\n",
    "            return [[0.9, 0.1]] * (len(X) if isinstance(X, list) else X.shape[0])\n",
    "        return [0.9, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_image_model(self):\n",
    "    \"\"\"Build and train the image classification model\"\"\"\n",
    "    print(\"Building image model...\")\n",
    "    # Filter data to include only image samples\n",
    "    image_data = [(img, label) for img, label in zip(self.data[\"images\"], self.data[\"labels\"]) if img is not None]\n",
    "    \n",
    "    if not image_data:\n",
    "        print(\"No image data available to train image model\")\n",
    "        return\n",
    "        \n",
    "    # Here we would implement a CNN or use a pre-trained model\n",
    "    # This is a placeholder since actual implementation would depend on your specific requirements\n",
    "    print(\"Image model would be implemented here with your specific image data\")\n",
    "    \n",
    "    # Create a properly picklable dummy model\n",
    "    self.image_model = DummyImageModel()  # Using the class defined outside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in ./myenv/lib/python3.13/site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./myenv/lib/python3.13/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./myenv/lib/python3.13/site-packages (from ipywidgets) (9.0.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./myenv/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in ./myenv/lib/python3.13/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in ./myenv/lib/python3.13/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in ./myenv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./myenv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./myenv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./myenv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./myenv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./myenv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./myenv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in ./myenv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./myenv/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./myenv/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./myenv/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./myenv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./myenv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./myenv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_url_checker():\n",
    "    \"\"\"Interactive function to check URLs\"\"\"\n",
    "    print(\"Narcotic Website Analyzer\")\n",
    "    print(\"Enter 'quit' to exit\")\n",
    "    \n",
    "    # Load classifier once\n",
    "    classifier = NarcoticWebsiteClassifier()\n",
    "    classifier.load_model(\"narcotic_classifier.pkl\")\n",
    "    \n",
    "    while True:\n",
    "        url = input(\"\\nEnter URL to check: \")\n",
    "        if url.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        if not url:\n",
    "            print(\"Please enter a valid URL\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Analyze the URL\n",
    "            print(f\"Analyzing: {url}\")\n",
    "            result = classifier.classify_website(url)\n",
    "            \n",
    "            # Display results\n",
    "            print(\"\\nResults:\")\n",
    "            print(f\"  Is narcotic: {result['is_narcotic']}\")\n",
    "            if 'confidence' in result:\n",
    "                print(f\"  Confidence: {result['confidence']:.2f}\")\n",
    "            if 'additional_signals' in result:\n",
    "                print(f\"  Additional signals:\")\n",
    "                for key, value in result['additional_signals'].items():\n",
    "                    print(f\"    - {key}: {value}\")\n",
    "            if 'error' in result:\n",
    "                print(f\"  Error: {result['error']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error testing URL: {e}\")\n",
    "            \n",
    "    print(\"Exiting URL checker\")\n",
    "\n",
    "# Run the interactive checker\n",
    "# interactive_url_checker()  # Uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image classification model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image model loaded\n",
      "Using device: cpu\n",
      "Loading data from: data\n",
      "Loaded 136 samples\n",
      "Building models...\n",
      "Building text model...\n",
      "Text model performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Building image model...\n",
      "Using ImageAnalyzer for image classification\n",
      "Building combined model...\n",
      "Model saved to narcotic_classifier.pkl\n",
      "Analyzing website: https://example.com\n",
      "Fetching website content...\n",
      "Extracting images from website...\n",
      "Found 0 images\n",
      "Analyzing text content...\n",
      "Analyzing images...\n",
      "\n",
      "Results for https://example.com:\n",
      "  Is narcotic: True\n",
      "  Confidence: 0.46\n",
      "  Additional signals: {'is_onion': False, 'keyword_match': False, 'suspicious_patterns': False, 'total_images': 0, 'suspicious_images': 0, 'suspicious_image_ratio': 0.0, 'text_confidence': np.float64(0.76)}\n",
      "Analyzing website: somefakesite.onion\n",
      "Onion URL detected. Using pre-configured proxy for Tor access...\n",
      "Analyzing text content...\n",
      "Analyzing images...\n",
      "\n",
      "Results for somefakesite.onion:\n",
      "  Is narcotic: True\n",
      "  Confidence: 0.57\n",
      "  Additional signals: {'is_onion': True, 'keyword_match': False, 'suspicious_patterns': False, 'total_images': 0, 'suspicious_images': 0, 'suspicious_image_ratio': 0.0, 'text_confidence': np.float64(0.79)}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import io\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoImageProcessor, AutoModelForImageClassification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class ImageAnalyzer:\n",
    "    \"\"\"Class to handle image analysis for narcotic content detection\"\"\"\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Load a pre-trained image classification model\n",
    "        try:\n",
    "            print(\"Loading image classification model...\")\n",
    "            self.processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "            self.model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\").to(self.device)\n",
    "            print(\"Image model loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image model: {e}\")\n",
    "            # Fallback to a basic model\n",
    "            self.model = None\n",
    "            self.processor = None\n",
    "    \n",
    "    def predict(self, image_data):\n",
    "        \"\"\"Analyze an image for suspicious content\n",
    "        \n",
    "        Args:\n",
    "            image_data: PIL Image or image bytes\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with analysis results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.model is None or self.processor is None:\n",
    "                return {'suspicious': False, 'confidence': 0.1}\n",
    "                \n",
    "            # Convert to PIL image if it's bytes\n",
    "            if isinstance(image_data, bytes):\n",
    "                image = Image.open(io.BytesIO(image_data))\n",
    "            elif isinstance(image_data, str) and os.path.exists(image_data):\n",
    "                image = Image.open(image_data)\n",
    "            else:\n",
    "                image = image_data\n",
    "                \n",
    "            # Check if image has suspicious dimensions or colors\n",
    "            # This is a simple heuristic - you would replace with your own logic\n",
    "            width, height = image.size\n",
    "            if width < 50 or height < 50:\n",
    "                return {'suspicious': False, 'confidence': 0.05}\n",
    "                \n",
    "            # Pre-process image for the model\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                \n",
    "            # Process outputs\n",
    "            # For demonstration, we're using a placeholder approach here\n",
    "            # You would typically look at specific classes relevant to narcotic content\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            \n",
    "            # For demonstration, we'll check if certain \"suspicious\" classes are high probability\n",
    "            # You would adapt this logic to your specific model and task\n",
    "            # This is just a placeholder - you'd use actual indices of relevant classes\n",
    "            suspicious_class_indices = [67, 401, 463]  # Placeholder indices\n",
    "            suspicious_probs = probabilities[0, suspicious_class_indices].sum().item()\n",
    "            \n",
    "            return {\n",
    "                'suspicious': suspicious_probs > 0.3,  # Threshold\n",
    "                'confidence': suspicious_probs\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing image: {e}\")\n",
    "            return {'suspicious': False, 'confidence': 0.0, 'error': str(e)}\n",
    "\n",
    "class NarcoticWebsiteClassifier:\n",
    "    def __init__(self, data_dir=\"data\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.text_model = None\n",
    "        self.image_model = None\n",
    "        self.combined_model = None\n",
    "        self.tokenizer = None\n",
    "        self.image_analyzer = ImageAnalyzer()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess all data from the data directory\"\"\"\n",
    "        print(\"Loading data from:\", self.data_dir)\n",
    "        self.data = {\n",
    "            \"text\": [],\n",
    "            \"images\": [],\n",
    "            \"urls\": [],\n",
    "            \"labels\": []\n",
    "        }\n",
    "        \n",
    "        # Recursively walk through all subdirectories\n",
    "        for root, dirs, files in os.walk(self.data_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_ext = os.path.splitext(file)[1].lower()\n",
    "                \n",
    "                # Process based on file type\n",
    "                if file_ext in ['.txt', '.csv', '.json']:\n",
    "                    self._process_text_file(file_path, file_ext)\n",
    "                elif file_ext in ['.jpg', '.jpeg', '.png']:\n",
    "                    self._process_image_file(file_path)\n",
    "        \n",
    "        print(f\"Loaded {len(self.data['labels'])} samples\")\n",
    "        return self.data\n",
    "    \n",
    "    def _process_text_file(self, file_path, file_ext):\n",
    "        \"\"\"Process text files based on their extension\"\"\"\n",
    "        try:\n",
    "            if file_ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    # Assuming each text file has content and a label (narcotic or not)\n",
    "                    # You'll need to adapt this based on your actual data structure\n",
    "                    is_narcotic = self._check_narcotic_keywords(content)\n",
    "                    self.data[\"text\"].append(content)\n",
    "                    self.data[\"labels\"].append(is_narcotic)\n",
    "                    self.data[\"urls\"].append(self._extract_url(content))\n",
    "                    self.data[\"images\"].append(None)\n",
    "                    \n",
    "            elif file_ext == '.csv':\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Adjust column names based on your CSV structure\n",
    "                if all(col in df.columns for col in ['content', 'url', 'is_narcotic']):\n",
    "                    for _, row in df.iterrows():\n",
    "                        self.data[\"text\"].append(row['content'])\n",
    "                        self.data[\"urls\"].append(row['url'])\n",
    "                        self.data[\"labels\"].append(row['is_narcotic'])\n",
    "                        self.data[\"images\"].append(None)\n",
    "                \n",
    "            elif file_ext == '.json':\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    json_data = json.load(f)\n",
    "                    # Process JSON based on your structure\n",
    "                    if isinstance(json_data, list):\n",
    "                        for item in json_data:\n",
    "                            if all(key in item for key in ['content', 'url', 'is_narcotic']):\n",
    "                                self.data[\"text\"].append(item['content'])\n",
    "                                self.data[\"urls\"].append(item['url'])\n",
    "                                self.data[\"labels\"].append(item['is_narcotic'])\n",
    "                                self.data[\"images\"].append(None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    def _process_image_file(self, file_path):\n",
    "        \"\"\"Process image files\"\"\"\n",
    "        try:\n",
    "            # For images, we'll need labels from a separate source or from the file path\n",
    "            # This is a placeholder - adapt to your data organization\n",
    "            parent_dir = os.path.basename(os.path.dirname(file_path))\n",
    "            is_narcotic = 'narcotic' in parent_dir.lower()\n",
    "            \n",
    "            self.data[\"images\"].append(file_path)\n",
    "            self.data[\"text\"].append(None)\n",
    "            self.data[\"urls\"].append(None)\n",
    "            self.data[\"labels\"].append(is_narcotic)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {file_path}: {e}\")\n",
    "    \n",
    "    def _check_narcotic_keywords(self, text):\n",
    "        \"\"\"Simple keyword check - replace with your actual logic\"\"\"\n",
    "        keywords = ['narcotic', 'drug', 'cocaine', 'heroin', 'mdma', 'lsd', \n",
    "                   'marijuana', 'cannabis', 'buy drugs', 'pills', 'opioid']\n",
    "        return any(keyword in text.lower() for keyword in keywords)\n",
    "    \n",
    "    def _extract_url(self, text):\n",
    "        \"\"\"Extract URL from text if present\"\"\"\n",
    "        url_pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+|(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\\.onion'\n",
    "        match = re.search(url_pattern, text)\n",
    "        return match.group(0) if match else None\n",
    "    \n",
    "    def build_models(self):\n",
    "        \"\"\"Build and train the models\"\"\"\n",
    "        print(\"Building models...\")\n",
    "        # 1. Text model using BERT\n",
    "        self._build_text_model()\n",
    "        \n",
    "        # 2. Image model\n",
    "        self._build_image_model()\n",
    "        \n",
    "        # 3. Combined model\n",
    "        self._build_combined_model()\n",
    "        \n",
    "    def _build_text_model(self):\n",
    "        \"\"\"Build and train the text classification model\"\"\"\n",
    "        print(\"Building text model...\")\n",
    "        # Filter data to include only text samples\n",
    "        text_data = [(text, label) for text, label in zip(self.data[\"text\"], self.data[\"labels\"]) if text is not None]\n",
    "        \n",
    "        if not text_data:\n",
    "            print(\"No text data available to train text model\")\n",
    "            return\n",
    "            \n",
    "        texts, labels = zip(*text_data)\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        text_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Extract features\n",
    "        features = []\n",
    "        for text in texts:\n",
    "            inputs = self.tokenizer(text, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = text_model(**inputs)\n",
    "            # Use CLS token as feature vector\n",
    "            features.append(outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten())\n",
    "        \n",
    "        # Train a classifier\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "        \n",
    "        self.text_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.text_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = self.text_model.predict(X_test)\n",
    "        print(\"Text model performance:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    def _build_image_model(self):\n",
    "        \"\"\"Build and train the image classification model\"\"\"\n",
    "        print(\"Building image model...\")\n",
    "        # Filter data to include only image samples\n",
    "        image_data = [(img, label) for img, label in zip(self.data[\"images\"], self.data[\"labels\"]) if img is not None]\n",
    "        \n",
    "        if not image_data:\n",
    "            print(\"No image data available to train image model\")\n",
    "            return\n",
    "            \n",
    "        # Here we simply use the ImageAnalyzer class\n",
    "        print(\"Using ImageAnalyzer for image classification\")\n",
    "        self.image_model = self.image_analyzer\n",
    "    \n",
    "    def _build_combined_model(self):\n",
    "        \"\"\"Build a model that combines text and image features\"\"\"\n",
    "        print(\"Building combined model...\")\n",
    "        # This would combine the outputs of the text and image models\n",
    "        # For now, just use the text model\n",
    "        self.combined_model = self.text_model\n",
    "    \n",
    "    def classify_website(self, url):\n",
    "        \"\"\"Classify a website as narcotic or not\"\"\"\n",
    "        print(f\"Analyzing website: {url}\")\n",
    "        \n",
    "        if self.text_model is None:\n",
    "            raise ValueError(\"Model has not been trained. Call build_models() first.\")\n",
    "        \n",
    "        # Check if it's an onion URL\n",
    "        is_onion = '.onion' in url\n",
    "        \n",
    "        try:\n",
    "            # For .onion URLs, we would need a Tor setup\n",
    "            if is_onion:\n",
    "                print(\"Onion URL detected. Using pre-configured proxy for Tor access...\")\n",
    "                # This is where you'd implement Tor proxy access\n",
    "                # For now, we'll use features that suggest it's likely narcotic\n",
    "                content = \"This is a placeholder for Tor hidden service content\"\n",
    "                images = []\n",
    "            else:\n",
    "                # For regular URLs, fetch the content\n",
    "                print(\"Fetching website content...\")\n",
    "                response = requests.get(url, timeout=10)\n",
    "                content = response.text\n",
    "                \n",
    "                # Extract images from the website\n",
    "                print(\"Extracting images from website...\")\n",
    "                images = self._extract_images(url, content)\n",
    "                print(f\"Found {len(images)} images\")\n",
    "                \n",
    "            # Extract text features\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            text_content = soup.get_text()\n",
    "            \n",
    "            # Text analysis\n",
    "            print(\"Analyzing text content...\")\n",
    "            text_result = self._analyze_text(text_content)\n",
    "            \n",
    "            # Image analysis\n",
    "            print(\"Analyzing images...\")\n",
    "            image_results = self._analyze_images(images)\n",
    "            \n",
    "            # Combined analysis\n",
    "            combined_result = self._combine_analyses(text_result, image_results, url, is_onion)\n",
    "            \n",
    "            return combined_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {url}: {e}\")\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"is_narcotic\": None,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _extract_images(self, base_url, html_content):\n",
    "        \"\"\"Extract image URLs from HTML content\"\"\"\n",
    "        images = []\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Find all img tags\n",
    "        img_tags = soup.find_all('img')\n",
    "        \n",
    "        for img in img_tags:\n",
    "            # Get the image URL\n",
    "            img_url = img.get('src')\n",
    "            if img_url:\n",
    "                # Make URL absolute if it's relative\n",
    "                if not img_url.startswith(('http://', 'https://')):\n",
    "                    img_url = urljoin(base_url, img_url)\n",
    "                \n",
    "                # Add to list\n",
    "                images.append(img_url)\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    def _analyze_text(self, text_content):\n",
    "        \"\"\"Analyze text content using the text model\"\"\"\n",
    "        # Tokenize and get features\n",
    "        inputs = self.tokenizer(text_content, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            text_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "            outputs = text_model(**inputs)\n",
    "        \n",
    "        # Use CLS token as feature vector\n",
    "        features = outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten().reshape(1, -1)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = self.text_model.predict(features)[0]\n",
    "        probability = self.text_model.predict_proba(features)[0][1]  # Probability of being narcotic\n",
    "        \n",
    "        # Additional signals\n",
    "        keyword_match = self._check_narcotic_keywords(text_content)\n",
    "        suspicious_patterns = self._check_suspicious_patterns(text_content)\n",
    "        \n",
    "        return {\n",
    "            \"is_narcotic\": prediction,\n",
    "            \"confidence\": probability,\n",
    "            \"keyword_match\": keyword_match,\n",
    "            \"suspicious_patterns\": suspicious_patterns\n",
    "        }\n",
    "    \n",
    "    def _analyze_images(self, image_urls):\n",
    "        \"\"\"Analyze images from the website\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for img_url in image_urls:\n",
    "            try:\n",
    "                # Fetch the image\n",
    "                response = requests.get(img_url, timeout=5)\n",
    "                if response.status_code == 200:\n",
    "                    img_data = response.content\n",
    "                    \n",
    "                    # Analyze the image\n",
    "                    analysis = self.image_analyzer.predict(img_data)\n",
    "                    \n",
    "                    # Add to results\n",
    "                    results.append({\n",
    "                        \"url\": img_url,\n",
    "                        \"suspicious\": analysis.get(\"suspicious\", False),\n",
    "                        \"confidence\": analysis.get(\"confidence\", 0.0)\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing image {img_url}: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _combine_analyses(self, text_result, image_results, url, is_onion):\n",
    "        \"\"\"Combine text and image analyses for final decision\"\"\"\n",
    "        # Calculate the percentage of suspicious images\n",
    "        total_images = len(image_results)\n",
    "        suspicious_images = sum(1 for img in image_results if img.get(\"suspicious\", False))\n",
    "        suspicious_image_ratio = suspicious_images / max(1, total_images)\n",
    "        \n",
    "        # Get text analysis results\n",
    "        text_is_narcotic = text_result.get(\"is_narcotic\", False)\n",
    "        text_confidence = text_result.get(\"confidence\", 0.0)\n",
    "        keyword_match = text_result.get(\"keyword_match\", False)\n",
    "        suspicious_patterns = text_result.get(\"suspicious_patterns\", False)\n",
    "        \n",
    "        # Determine overall probability\n",
    "        # This is a simple weighted approach - you can make this more sophisticated\n",
    "        overall_confidence = (\n",
    "            0.6 * text_confidence + \n",
    "            0.3 * suspicious_image_ratio + \n",
    "            0.1 * (1.0 if is_onion else 0.0)\n",
    "        )\n",
    "        \n",
    "        # Make final decision\n",
    "        # Considered narcotic if any of these are true\n",
    "        is_narcotic = (\n",
    "            text_is_narcotic or\n",
    "            suspicious_image_ratio > 0.3 or\n",
    "            (is_onion and (keyword_match or suspicious_patterns))\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"is_narcotic\": is_narcotic,\n",
    "            \"confidence\": overall_confidence,\n",
    "            \"additional_signals\": {\n",
    "                \"is_onion\": is_onion,\n",
    "                \"keyword_match\": keyword_match,\n",
    "                \"suspicious_patterns\": suspicious_patterns,\n",
    "                \"total_images\": total_images,\n",
    "                \"suspicious_images\": suspicious_images,\n",
    "                \"suspicious_image_ratio\": suspicious_image_ratio,\n",
    "                \"text_confidence\": text_confidence\n",
    "            },\n",
    "            \"image_analysis\": image_results if total_images > 0 else \"No images found\"\n",
    "        }\n",
    "    \n",
    "    def _check_suspicious_patterns(self, content, url=None):\n",
    "        \"\"\"Check for patterns common in narcotic websites\"\"\"\n",
    "        patterns = [\n",
    "            r'bitcoin|btc|monero|xmr|cryptocurrency',  # Payment methods\n",
    "            r'escrow|vendor|marketplace',              # Marketplace terminology\n",
    "            r'anonymous|encrypted|secure',             # Security terms\n",
    "            r'shipping|delivery|tracking',             # Shipping terms\n",
    "            r'telegram|wickr|signal'                   # Communication apps\n",
    "        ]\n",
    "        \n",
    "        return any(re.search(pattern, content.lower()) for pattern in patterns)\n",
    "    \n",
    "    def save_model(self, path=\"narcotic_classifier.pkl\"):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        if self.text_model is None:\n",
    "            raise ValueError(\"No model to save. Train the model first.\")\n",
    "            \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'text_model': self.text_model,\n",
    "                'combined_model': self.combined_model\n",
    "            }, f)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    def load_model(self, path=\"narcotic_classifier.pkl\"):\n",
    "        \"\"\"Load a saved model\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            models = pickle.load(f)\n",
    "            self.text_model = models['text_model']\n",
    "            self.combined_model = models['combined_model']\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        \n",
    "        # Load the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Initialize image analyzer\n",
    "        self.image_analyzer = ImageAnalyzer()\n",
    "        self.image_model = self.image_analyzer\n",
    "        \n",
    "        print(\"Model loaded successfully\")\n",
    "        \n",
    "# Interactive URL testing function\n",
    "def test_url(url):\n",
    "    \"\"\"Test a specific URL with the trained classifier\"\"\"\n",
    "    try:\n",
    "        # Load the trained classifier\n",
    "        classifier = NarcoticWebsiteClassifier()\n",
    "        classifier.load_model(\"narcotic_classifier.pkl\")\n",
    "        \n",
    "        # Analyze the URL\n",
    "        print(f\"Analyzing: {url}\")\n",
    "        result = classifier.classify_website(url)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\nResults:\")\n",
    "        print(f\"  Is narcotic: {result['is_narcotic']}\")\n",
    "        if 'confidence' in result:\n",
    "            print(f\"  Confidence: {result['confidence']:.2f}\")\n",
    "        if 'additional_signals' in result:\n",
    "            print(f\"  Additional signals:\")\n",
    "            for key, value in result['additional_signals'].items():\n",
    "                print(f\"    - {key}: {value}\")\n",
    "        \n",
    "        # Display image analysis if available\n",
    "        if 'image_analysis' in result and result['image_analysis'] != \"No images found\":\n",
    "            print(\"\\n  Image Analysis:\")\n",
    "            for i, img_result in enumerate(result['image_analysis']):\n",
    "                print(f\"    Image {i+1}: {img_result['url']}\")\n",
    "                print(f\"      - Suspicious: {img_result['suspicious']}\")\n",
    "                print(f\"      - Confidence: {img_result['confidence']:.2f}\")\n",
    "                \n",
    "        if 'error' in result:\n",
    "            print(f\"  Error: {result['error']}\")\n",
    "            \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing URL: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create a widget-based interface\n",
    "def create_interactive_interface():\n",
    "    \"\"\"Create an interactive widget-based interface\"\"\"\n",
    "    try:\n",
    "        import ipywidgets as widgets\n",
    "        from IPython.display import display\n",
    "\n",
    "        # Create input widget\n",
    "        url_input = widgets.Text(\n",
    "            value='https://example.com',\n",
    "            placeholder='Enter URL to check',\n",
    "            description='URL:',\n",
    "            disabled=False,\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='80%')\n",
    "        )\n",
    "\n",
    "        # Create button\n",
    "        check_button = widgets.Button(\n",
    "            description='Check Website',\n",
    "            disabled=False,\n",
    "            button_style='primary',\n",
    "            tooltip='Click to analyze the URL'\n",
    "        )\n",
    "\n",
    "        # Create output area\n",
    "        output = widgets.Output()\n",
    "\n",
    "        # Button click handler\n",
    "        def on_button_clicked(b):\n",
    "            with output:\n",
    "                output.clear_output()\n",
    "                url = url_input.value\n",
    "                if url:\n",
    "                    test_url(url)\n",
    "                else:\n",
    "                    print(\"Please enter a URL\")\n",
    "\n",
    "        # Connect button to function\n",
    "        check_button.on_click(on_button_clicked)\n",
    "\n",
    "        # Display the UI\n",
    "        display(widgets.VBox([widgets.Label(\"Narcotic Website Analyzer\"), url_input, check_button, output]))\n",
    "    except ImportError:\n",
    "        print(\"ipywidgets not available. Use test_url() function directly.\")\n",
    "        \n",
    "# Example usage\n",
    "def main():\n",
    "    # Initialize the classifier\n",
    "    classifier = NarcoticWebsiteClassifier(data_dir=\"data\")\n",
    "    \n",
    "    # Load and preprocess the data\n",
    "    classifier.load_data()\n",
    "    \n",
    "    # Build and train the models\n",
    "    classifier.build_models()\n",
    "    \n",
    "    try:\n",
    "        # Save the model\n",
    "        classifier.save_model()\n",
    "        \n",
    "        # Test the classifier on some example URLs\n",
    "        test_urls = [\n",
    "            \"https://example.com\",\n",
    "            \"somefakesite.onion\"\n",
    "        ]\n",
    "        \n",
    "        for url in test_urls:\n",
    "            result = classifier.classify_website(url)\n",
    "            print(f\"\\nResults for {url}:\")\n",
    "            print(f\"  Is narcotic: {result['is_narcotic']}\")\n",
    "            if 'confidence' in result:\n",
    "                print(f\"  Confidence: {result['confidence']:.2f}\")\n",
    "            if 'additional_signals' in result:\n",
    "                print(f\"  Additional signals: {result['additional_signals']}\")\n",
    "            if 'error' in result:\n",
    "                print(f\"  Error: {result['error']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./myenv/lib/python3.13/site-packages (4.49.0)\n",
      "Requirement already satisfied: pillow in ./myenv/lib/python3.13/site-packages (11.1.0)\n",
      "Requirement already satisfied: torch in ./myenv/lib/python3.13/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.13/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./myenv/lib/python3.13/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./myenv/lib/python3.13/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./myenv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./myenv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./myenv/lib/python3.13/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./myenv/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./myenv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./myenv/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./myenv/lib/python3.13/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./myenv/lib/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./myenv/lib/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./myenv/lib/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./myenv/lib/python3.13/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./myenv/lib/python3.13/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./myenv/lib/python3.13/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./myenv/lib/python3.13/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./myenv/lib/python3.13/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./myenv/lib/python3.13/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./myenv/lib/python3.13/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./myenv/lib/python3.13/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./myenv/lib/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./myenv/lib/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./myenv/lib/python3.13/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in ./myenv/lib/python3.13/site-packages (from torch) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./myenv/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.13/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers pillow torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77add8057ad4f49aa055e8956175490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Narcotic Website Analyzer'), Text(value='https://example.com', description='URL:',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_interactive_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2283150405c14328a93ad5de70fd9615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Narcotic Website Analyzer'), Text(value='https://example.com', description='URL:',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_interactive_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

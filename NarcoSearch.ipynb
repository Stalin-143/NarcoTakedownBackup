{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image classification model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image model loaded\n",
      "Using device: cpu\n",
      "Loading data from: data\n",
      "Error processing data/ly75dbzixy7hlp663j32xo4dtoiikm6bxb53jvivqkpo6jwppptx3sad_onion/table_1.csv: No columns to parse from file\n",
      "Error processing data/ly75dbzi2yrzkugzfrgn4zbp2unpjpyth3qopbfthscunlpdfypi3lqd_onion/table_1.csv: No columns to parse from file\n",
      "Loaded 461 samples\n",
      "Building models...\n",
      "Building text model...\n",
      "Text model performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00         1\n",
      "        True       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        12\n",
      "   macro avg       1.00      1.00      1.00        12\n",
      "weighted avg       1.00      1.00      1.00        12\n",
      "\n",
      "Building image model...\n",
      "Using ImageAnalyzer for image classification\n",
      "Building combined model...\n",
      "Model saved to narcotic_classifier.pkl\n",
      "Error during execution: name 'test_urls' is not defined\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600d03f13963400cb208cac1a1533be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Narcotic Website Analyzer'), Text(value='https://example.com', description='URL:',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import io\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoImageProcessor, AutoModelForImageClassification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class ImageAnalyzer:\n",
    "    \"\"\"Class to handle image analysis for narcotic content detection\"\"\"\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Load a pre-trained image classification model\n",
    "        try:\n",
    "            print(\"Loading image classification model...\")\n",
    "            self.processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "            self.model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\").to(self.device)\n",
    "            print(\"Image model loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image model: {e}\")\n",
    "            # Fallback to a basic model\n",
    "            self.model = None\n",
    "            self.processor = None\n",
    "    \n",
    "    def predict(self, image_data):\n",
    "        \"\"\"Analyze an image for suspicious content\n",
    "        \n",
    "        Args:\n",
    "            image_data: PIL Image or image bytes\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with analysis results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.model is None or self.processor is None:\n",
    "                return {'suspicious': False, 'confidence': 0.1}\n",
    "                \n",
    "            # Convert to PIL image if it's bytes\n",
    "            if isinstance(image_data, bytes):\n",
    "                image = Image.open(io.BytesIO(image_data))\n",
    "            elif isinstance(image_data, str) and os.path.exists(image_data):\n",
    "                image = Image.open(image_data)\n",
    "            else:\n",
    "                image = image_data\n",
    "                \n",
    "            # Check if image has suspicious dimensions or colors\n",
    "            # This is a simple heuristic - you would replace with your own logic\n",
    "            width, height = image.size\n",
    "            if width < 50 or height < 50:\n",
    "                return {'suspicious': False, 'confidence': 0.05}\n",
    "                \n",
    "            # Pre-process image for the model\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                \n",
    "            # Process outputs\n",
    "            # For demonstration, we're using a placeholder approach here\n",
    "            # You would typically look at specific classes relevant to narcotic content\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            \n",
    "            # For demonstration, we'll check if certain \"suspicious\" classes are high probability\n",
    "            # You would adapt this logic to your specific model and task\n",
    "            # This is just a placeholder - you'd use actual indices of relevant classes\n",
    "            suspicious_class_indices = [67, 401, 463]  # Placeholder indices\n",
    "            suspicious_probs = probabilities[0, suspicious_class_indices].sum().item()\n",
    "            \n",
    "            return {\n",
    "                'suspicious': suspicious_probs > 0.3,  # Threshold\n",
    "                'confidence': suspicious_probs\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing image: {e}\")\n",
    "            return {'suspicious': False, 'confidence': 0.0, 'error': str(e)}\n",
    "\n",
    "class NarcoticWebsiteClassifier:\n",
    "    def __init__(self, data_dir=\"data\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.text_model = None\n",
    "        self.image_model = None\n",
    "        self.combined_model = None\n",
    "        self.tokenizer = None\n",
    "        self.image_analyzer = ImageAnalyzer()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess all data from the data directory\"\"\"\n",
    "        print(\"Loading data from:\", self.data_dir)\n",
    "        self.data = {\n",
    "            \"text\": [],\n",
    "            \"images\": [],\n",
    "            \"urls\": [],\n",
    "            \"labels\": []\n",
    "        }\n",
    "        \n",
    "        # Recursively walk through all subdirectories\n",
    "        for root, dirs, files in os.walk(self.data_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_ext = os.path.splitext(file)[1].lower()\n",
    "                \n",
    "                # Process based on file type\n",
    "                if file_ext in ['.txt', '.csv', '.json']:\n",
    "                    self._process_text_file(file_path, file_ext)\n",
    "                elif file_ext in ['.jpg', '.jpeg', '.png']:\n",
    "                    self._process_image_file(file_path)\n",
    "        \n",
    "        print(f\"Loaded {len(self.data['labels'])} samples\")\n",
    "        return self.data\n",
    "    \n",
    "    def _process_text_file(self, file_path, file_ext):\n",
    "        \"\"\"Process text files based on their extension\"\"\"\n",
    "        try:\n",
    "            if file_ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    # Assuming each text file has content and a label (narcotic or not)\n",
    "                    # You'll need to adapt this based on your actual data structure\n",
    "                    is_narcotic = self._check_narcotic_keywords(content)\n",
    "                    self.data[\"text\"].append(content)\n",
    "                    self.data[\"labels\"].append(is_narcotic)\n",
    "                    self.data[\"urls\"].append(self._extract_url(content))\n",
    "                    self.data[\"images\"].append(None)\n",
    "                    \n",
    "            elif file_ext == '.csv':\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Adjust column names based on your CSV structure\n",
    "                if all(col in df.columns for col in ['content', 'url', 'is_narcotic']):\n",
    "                    for _, row in df.iterrows():\n",
    "                        self.data[\"text\"].append(row['content'])\n",
    "                        self.data[\"urls\"].append(row['url'])\n",
    "                        self.data[\"labels\"].append(row['is_narcotic'])\n",
    "                        self.data[\"images\"].append(None)\n",
    "                \n",
    "            elif file_ext == '.json':\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    json_data = json.load(f)\n",
    "                    # Process JSON based on your structure\n",
    "                    if isinstance(json_data, list):\n",
    "                        for item in json_data:\n",
    "                            if all(key in item for key in ['content', 'url', 'is_narcotic']):\n",
    "                                self.data[\"text\"].append(item['content'])\n",
    "                                self.data[\"urls\"].append(item['url'])\n",
    "                                self.data[\"labels\"].append(item['is_narcotic'])\n",
    "                                self.data[\"images\"].append(None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    def _process_image_file(self, file_path):\n",
    "        \"\"\"Process image files\"\"\"\n",
    "        try:\n",
    "            # For images, we'll need labels from a separate source or from the file path\n",
    "            # This is a placeholder - adapt to your data organization\n",
    "            parent_dir = os.path.basename(os.path.dirname(file_path))\n",
    "            is_narcotic = 'narcotic' in parent_dir.lower()\n",
    "            \n",
    "            self.data[\"images\"].append(file_path)\n",
    "            self.data[\"text\"].append(None)\n",
    "            self.data[\"urls\"].append(None)\n",
    "            self.data[\"labels\"].append(is_narcotic)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {file_path}: {e}\")\n",
    "    \n",
    "    def _check_narcotic_keywords(self, text):\n",
    "        \"\"\"Simple keyword check - replace with your actual logic\"\"\"\n",
    "        keywords = ['narcotic', 'drug', 'cocaine', 'heroin', 'mdma', 'lsd', \n",
    "                   'marijuana', 'cannabis', 'buy drugs', 'pills', 'opioid'\n",
    "                    'heroin', 'fentanyl', 'carfentanil', 'morphine', 'codeine', 'oxycodone', 'hydrocodone', 'oxycontin',\n",
    "                    'lisdexamfetamine', 'benzedrine', 'dexosyn', 'desoxyn', 'white', 'snow', 'blow', 'flake',\n",
    "                    'yayo', 'yeyo', 'rock', 'hard', 'nose candy', 'tina', 'crank', 'go', 'go fast', 'uppers',\n",
    "                    'stimulants', 'stims', 'tweak', 'tweaking', 'gak', 'yay', 'yola', 'cola', 'powder', 'base',\n",
    "                    'freebase', 'addy', 'addies', 'study drug', 'study aid', 'bennies', 'pep pills', 'dexies',\n",
    "                    'methylone', 'bath salts', 'cathinones', 'khat', 'qat', 'modafinil', 'provigil', 'armodafinil',\n",
    "                    'nuvigil', 'ephedrine', 'pseudoephedrine', 'sudafed', 'shards', 'christina', 'shabu', \n",
    "                    'jane', 'green', 'trees', 'flower', 'nugs', 'buds', '420', 'four twenty', 'four-twenty']\n",
    "                               \n",
    "                \n",
    "        return any(keyword in text.lower() for keyword in keywords)\n",
    "    \n",
    "    def _extract_url(self, text):\n",
    "        \"\"\"Extract URL from text if present\"\"\"\n",
    "        url_pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+|(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\\.onion'\n",
    "        match = re.search(url_pattern, text)\n",
    "        return match.group(0) if match else None\n",
    "    \n",
    "    def build_models(self):\n",
    "        \"\"\"Build and train the models\"\"\"\n",
    "        print(\"Building models...\")\n",
    "        # 1. Text model using BERT\n",
    "        self._build_text_model()\n",
    "        \n",
    "        # 2. Image model\n",
    "        self._build_image_model()\n",
    "        \n",
    "        # 3. Combined model\n",
    "        self._build_combined_model()\n",
    "        \n",
    "    def _build_text_model(self):\n",
    "        \"\"\"Build and train the text classification model\"\"\"\n",
    "        print(\"Building text model...\")\n",
    "        # Filter data to include only text samples\n",
    "        text_data = [(text, label) for text, label in zip(self.data[\"text\"], self.data[\"labels\"]) if text is not None]\n",
    "        \n",
    "        if not text_data:\n",
    "            print(\"No text data available to train text model\")\n",
    "            return\n",
    "            \n",
    "        texts, labels = zip(*text_data)\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        text_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Extract features\n",
    "        features = []\n",
    "        for text in texts:\n",
    "            inputs = self.tokenizer(text, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = text_model(**inputs)\n",
    "            # Use CLS token as feature vector\n",
    "            features.append(outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten())\n",
    "        \n",
    "        # Train a classifier\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "        \n",
    "        self.text_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.text_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = self.text_model.predict(X_test)\n",
    "        print(\"Text model performance:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    def _build_image_model(self):\n",
    "        \"\"\"Build and train the image classification model\"\"\"\n",
    "        print(\"Building image model...\")\n",
    "        # Filter data to include only image samples\n",
    "        image_data = [(img, label) for img, label in zip(self.data[\"images\"], self.data[\"labels\"]) if img is not None]\n",
    "        \n",
    "        if not image_data:\n",
    "            print(\"No image data available to train image model\")\n",
    "            return\n",
    "            \n",
    "        # Here we simply use the ImageAnalyzer class\n",
    "        print(\"Using ImageAnalyzer for image classification\")\n",
    "        self.image_model = self.image_analyzer\n",
    "    \n",
    "    def _build_combined_model(self):\n",
    "        \"\"\"Build a model that combines text and image features\"\"\"\n",
    "        print(\"Building combined model...\")\n",
    "        # This would combine the outputs of the text and image models\n",
    "        # For now, just use the text model\n",
    "        self.combined_model = self.text_model\n",
    "    \n",
    "    def classify_website(self, url):\n",
    "        \"\"\"Classify a website as narcotic or not\"\"\"\n",
    "        print(f\"Analyzing website: {url}\")\n",
    "        \n",
    "        if self.text_model is None:\n",
    "            raise ValueError(\"Model has not been trained. Call build_models() first.\")\n",
    "        \n",
    "        # Check if it's an onion URL\n",
    "        is_onion = '.onion' in url\n",
    "        \n",
    "        try:\n",
    "            # For .onion URLs, we would need a Tor setup\n",
    "            if is_onion:\n",
    "                print(\"Onion URL detected. Using pre-configured proxy for Tor access...\")\n",
    "                # This is where you'd implement Tor proxy access\n",
    "                # For now, we'll use features that suggest it's likely narcotic\n",
    "                content = \"This is a placeholder for Tor hidden service content\"\n",
    "                images = []\n",
    "            else:\n",
    "                # For regular URLs, fetch the content\n",
    "                print(\"Fetching website content...\")\n",
    "                response = requests.get(url, timeout=10)\n",
    "                content = response.text\n",
    "                \n",
    "                # Extract images from the website\n",
    "                print(\"Extracting images from website...\")\n",
    "                images = self._extract_images(url, content)\n",
    "                print(f\"Found {len(images)} images\")\n",
    "                \n",
    "            # Extract text features\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            text_content = soup.get_text()\n",
    "            \n",
    "            # Text analysis\n",
    "            print(\"Analyzing text content...\")\n",
    "            text_result = self._analyze_text(text_content)\n",
    "            \n",
    "            # Image analysis\n",
    "            print(\"Analyzing images...\")\n",
    "            image_results = self._analyze_images(images)\n",
    "            \n",
    "            # Combined analysis\n",
    "            combined_result = self._combine_analyses(text_result, image_results, url, is_onion)\n",
    "            \n",
    "            return combined_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {url}: {e}\")\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"is_narcotic\": None,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _extract_images(self, base_url, html_content):\n",
    "        \"\"\"Extract image URLs from HTML content\"\"\"\n",
    "        images = []\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Find all img tags\n",
    "        img_tags = soup.find_all('img')\n",
    "        \n",
    "        for img in img_tags:\n",
    "            # Get the image URL\n",
    "            img_url = img.get('src')\n",
    "            if img_url:\n",
    "                # Make URL absolute if it's relative\n",
    "                if not img_url.startswith(('http://', 'https://')):\n",
    "                    img_url = urljoin(base_url, img_url)\n",
    "                \n",
    "                # Add to list\n",
    "                images.append(img_url)\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    def _analyze_text(self, text_content):\n",
    "        \"\"\"Analyze text content using the text model\"\"\"\n",
    "        # Tokenize and get features\n",
    "        inputs = self.tokenizer(text_content, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            text_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "            outputs = text_model(**inputs)\n",
    "        \n",
    "        # Use CLS token as feature vector\n",
    "        features = outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten().reshape(1, -1)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = self.text_model.predict(features)[0]\n",
    "        probability = self.text_model.predict_proba(features)[0][1]  # Probability of being narcotic\n",
    "        \n",
    "        # Additional signals\n",
    "        keyword_match = self._check_narcotic_keywords(text_content)\n",
    "        suspicious_patterns = self._check_suspicious_patterns(text_content)\n",
    "        \n",
    "        return {\n",
    "            \"is_narcotic\": prediction,\n",
    "            \"confidence\": probability,\n",
    "            \"keyword_match\": keyword_match,\n",
    "            \"suspicious_patterns\": suspicious_patterns\n",
    "        }\n",
    "    \n",
    "    def _analyze_images(self, image_urls):\n",
    "        \"\"\"Analyze images from the website\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for img_url in image_urls:\n",
    "            try:\n",
    "                # Fetch the image\n",
    "                response = requests.get(img_url, timeout=5)\n",
    "                if response.status_code == 200:\n",
    "                    img_data = response.content\n",
    "                    \n",
    "                    # Analyze the image\n",
    "                    analysis = self.image_analyzer.predict(img_data)\n",
    "                    \n",
    "                    # Add to results\n",
    "                    results.append({\n",
    "                        \"url\": img_url,\n",
    "                        \"suspicious\": analysis.get(\"suspicious\", False),\n",
    "                        \"confidence\": analysis.get(\"confidence\", 0.0)\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing image {img_url}: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _combine_analyses(self, text_result, image_results, url, is_onion):\n",
    "        \"\"\"Combine text and image analyses for final decision\"\"\"\n",
    "        # Calculate the percentage of suspicious images\n",
    "        total_images = len(image_results)\n",
    "        suspicious_images = sum(1 for img in image_results if img.get(\"suspicious\", False))\n",
    "        suspicious_image_ratio = suspicious_images / max(1, total_images)\n",
    "        \n",
    "        # Get text analysis results\n",
    "        text_is_narcotic = text_result.get(\"is_narcotic\", False)\n",
    "        text_confidence = text_result.get(\"confidence\", 0.0)\n",
    "        keyword_match = text_result.get(\"keyword_match\", False)\n",
    "        suspicious_patterns = text_result.get(\"suspicious_patterns\", False)\n",
    "        \n",
    "        # Determine overall probability\n",
    "        # This is a simple weighted approach - you can make this more sophisticated\n",
    "        overall_confidence = (\n",
    "            0.6 * text_confidence + \n",
    "            0.3 * suspicious_image_ratio + \n",
    "            0.1 * (1.0 if is_onion else 0.0)\n",
    "        )\n",
    "        \n",
    "        # Make final decision\n",
    "        # Considered narcotic if any of these are true\n",
    "        is_narcotic = (\n",
    "            text_is_narcotic or\n",
    "            suspicious_image_ratio > 0.3 or\n",
    "            (is_onion and (keyword_match or suspicious_patterns))\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"is_narcotic\": is_narcotic,\n",
    "            \"confidence\": overall_confidence,\n",
    "            \"additional_signals\": {\n",
    "                \"is_onion\": is_onion,\n",
    "                \"keyword_match\": keyword_match,\n",
    "                \"suspicious_patterns\": suspicious_patterns,\n",
    "                \"total_images\": total_images,\n",
    "                \"suspicious_images\": suspicious_images,\n",
    "                \"suspicious_image_ratio\": suspicious_image_ratio,\n",
    "                \"text_confidence\": text_confidence\n",
    "            },\n",
    "            \"image_analysis\": image_results if total_images > 0 else \"No images found\"\n",
    "        }\n",
    "    \n",
    "    def _check_suspicious_patterns(self, content, url=None):\n",
    "        \"\"\"Check for patterns common in narcotic websites\"\"\"\n",
    "        patterns = [\n",
    "            r'bitcoin|btc|monero|xmr|cryptocurrency',  # Payment methods\n",
    "            r'escrow|vendor|marketplace',              # Marketplace terminology\n",
    "            r'anonymous|encrypted|secure',             # Security terms\n",
    "            r'shipping|delivery|tracking',             # Shipping terms\n",
    "            r'telegram|wickr|signal' \n",
    "             # Payment methods\n",
    "            r'bitcoin|btc|monero|xmr|cryptocurrency|crypto|eth|ethereum|ltc|litecoin|xrp|ripple|dash|zcash|zec',\n",
    "            r'wallet|transaction|blockchain|mixer|tumbler|mixing|tumbling|atomic swap|p2p|peer[\\s-]to[\\s-]peer',\n",
    "            r'coinbase|binance|kraken|kucoin|localbitcoins|paxful|bisq|exodus|metamask|cold storage',\n",
    "            r'lightning network|segwit|unconfirmed|confirmations|block explorer|private key|public key',\n",
    "            r'non[\\s-]custodial|seed phrase|recovery phrase|backup|2fa|two[\\s-]factor|authentication',\n",
    "            r'escrow|multisig|multi[\\s-]signature|finalize early|fe|auto[\\s-]finalize|pgp|payment processor',\n",
    "            r'cash[\\s-]app|venmo|paypal|western union|money order|gift card|prepaid|voucher|vanilla|paysafecard',\n",
    "            r'cashless|anonymous payment|secure payment|untraceable|no kyc|no verification',\n",
    "            \n",
    "            # Marketplace terminology\n",
    "            r'escrow|vendor|marketplace|market|bazaar|shop|store|mall|emporium|exchange|darknet|darkweb',\n",
    "            r'verified vendor|trusted seller|trusted vendor|vendor rating|feedback|review|rating system',\n",
    "            r'verified buyer|market admin|dispute|resolution|support ticket|refund policy|reship policy',\n",
    "            r'listing|product page|description|category|subcategory|inventory|stock|restock|out of stock',\n",
    "            r'vendor shop|vendor page|vendor profile|pgp key|pgp verification|vendor level|trust level',\n",
    "            r'featured listing|hot listing|new arrival|best seller|top rated|highly rated|verified purchase',\n",
    "            r'market rules|terms of service|tos|faq|frequently asked|vendor bond|vendor fee|buyer protection',\n",
    "            r'market mirror|alternative link|official link|phishing|scam|exit scam|selective scam|legit check',\n",
    "            \n",
    "            # Security terms\n",
    "            r'anonymous|encrypted|secure|private|hidden|concealed|stealth|covert|discreet|clandestine|secret',\n",
    "            r'tor|onion|tails|whonix|i2p|freenet|vpn|proxy|bridge|relay|node|encryption|decryption|cipher',\n",
    "            r'pgp|gpg|public key|private key|aes|rsa|elliptic curve|end[\\s-]to[\\s-]end|e2e|opsec|persec',\n",
    "            r'security|privacy|anonymity|pseudonym|alias|burner|throwaway|temporary|compartmentalization',\n",
    "            r'key verification|signature verification|hash verification|checksum|sha256|md5|fingerprint',\n",
    "            r'2fa|two[\\s-]factor|authentication|captcha|verification|authorization|secure login|session',\n",
    "            r'no[\\s-]logs?|zero[\\s-]logs?|no[\\s-]records?|no[\\s-]tracking|surveillance|monitoring|compromise',\n",
    "            r'secure connection|encrypted connection|secure channel|secure communication|secure messaging',\n",
    "            \n",
    "            # Shipping terms\n",
    "            r'shipping|delivery|tracking|package|parcel|mail|post|courier|dispatch|shipment|shipping method',\n",
    "            r'express|priority|standard|economy|overnight|next[\\s-]day|same[\\s-]day|2[\\s-]day|3[\\s-]day',\n",
    "            r'domestic|international|worldwide|global|regional|local|cross[\\s-]border|customs|border control',\n",
    "            r'stealth|decoy|vacuum sealed|moisture barrier|mylar|visual barrier|odor proof|smell proof',\n",
    "            r'drop|drop off|drop location|safe location|safe address|pickup|collection point|p\\.o\\. box',\n",
    "            r'shipping time|delivery time|transit time|estimated arrival|eta|shipping delay|processing time',\n",
    "            r'carrier|usps|ups|fedex|dhl|royal mail|canada post|auspost|deutsche post|postal service',\n",
    "            r'signature|signature required|no signature|shipping label|tracking number|customs declaration',\n",
    "            \n",
    "            # Communication apps\n",
    "            r'telegram|wickr|signal|session|element|matrix|jabber|xmpp|riot|wire|threema|status|briar',\n",
    "            r'encrypted chat|secure messaging|private messaging|self[\\s-]destructing|disappearing messages',\n",
    "            r'otr|off[\\s-]the[\\s-]record|e2ee|end[\\s-]to[\\s-]end encryption|forward secrecy|perfect forward',\n",
    "            r'burner phone|burner account|throwaway account|temporary email|temp mail|protonmail|tutanota',\n",
    "            r'secure email|encrypted email|private email|anonymous email|email service|email provider',\n",
    "            r'messaging app|chat app|communication platform|secure platform|anonymous platform|private app',\n",
    "            r'contact method|contact details|contact information|reach out|get in touch|direct message',\n",
    "            r'username|handle|contact id|address|dm|pm|private message|direct message|secure channel',\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        return any(re.search(pattern, content.lower()) for pattern in patterns)\n",
    "    \n",
    "    def save_model(self, path=\"narcotic_classifier.pkl\"):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        if self.text_model is None:\n",
    "            raise ValueError(\"No model to save. Train the model first.\")\n",
    "            \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'text_model': self.text_model,\n",
    "                'combined_model': self.combined_model\n",
    "            }, f)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    def load_model(self, path=\"narcotic_classifier.pkl\"):\n",
    "        \"\"\"Load a saved model\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            models = pickle.load(f)\n",
    "            self.text_model = models['text_model']\n",
    "            self.combined_model = models['combined_model']\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        \n",
    "        # Load the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Initialize image analyzer\n",
    "        self.image_analyzer = ImageAnalyzer()\n",
    "        self.image_model = self.image_analyzer\n",
    "        \n",
    "        print(\"Model loaded successfully\")\n",
    "        \n",
    "# Interactive URL testing function\n",
    "def test_url(url):\n",
    "    \"\"\"Test a specific URL with the trained classifier\"\"\"\n",
    "    try:\n",
    "        # Load the trained classifier\n",
    "        classifier = NarcoticWebsiteClassifier()\n",
    "        classifier.load_model(\"narcotic_classifier.pkl\")\n",
    "        \n",
    "        # Analyze the URL\n",
    "        print(f\"Analyzing: {url}\")\n",
    "        result = classifier.classify_website(url)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\nResults:\")\n",
    "        print(f\"  Is narcotic: {result['is_narcotic']}\")\n",
    "        if 'confidence' in result:\n",
    "            print(f\"  Confidence: {result['confidence']:.2f}\")\n",
    "        if 'additional_signals' in result:\n",
    "            print(f\"  Additional signals:\")\n",
    "            for key, value in result['additional_signals'].items():\n",
    "                print(f\"    - {key}: {value}\")\n",
    "        \n",
    "        # Display image analysis if available\n",
    "        if 'image_analysis' in result and result['image_analysis'] != \"No images found\":\n",
    "            print(\"\\n  Image Analysis:\")\n",
    "            for i, img_result in enumerate(result['image_analysis']):\n",
    "                print(f\"    Image {i+1}: {img_result['url']}\")\n",
    "                print(f\"      - Suspicious: {img_result['suspicious']}\")\n",
    "                print(f\"      - Confidence: {img_result['confidence']:.2f}\")\n",
    "                \n",
    "        if 'error' in result:\n",
    "            print(f\"  Error: {result['error']}\")\n",
    "            \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing URL: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create a widget-based interface\n",
    "def create_interactive_interface():\n",
    "    \"\"\"Create an interactive widget-based interface\"\"\"\n",
    "    try:\n",
    "        import ipywidgets as widgets\n",
    "        from IPython.display import display\n",
    "\n",
    "        # Create input widget\n",
    "        url_input = widgets.Text(\n",
    "            value='https://example.com',\n",
    "            placeholder='Enter URL to check',\n",
    "            description='URL:',\n",
    "            disabled=False,\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='80%')\n",
    "        )\n",
    "\n",
    "        # Create button\n",
    "        check_button = widgets.Button(\n",
    "            description='Check Website',\n",
    "            disabled=False,\n",
    "            button_style='primary',\n",
    "            tooltip='Click to analyze the URL'\n",
    "        )\n",
    "\n",
    "        # Create output area\n",
    "        output = widgets.Output()\n",
    "\n",
    "        # Button click handler\n",
    "        def on_button_clicked(b):\n",
    "            with output:\n",
    "                output.clear_output()\n",
    "                url = url_input.value\n",
    "                if url:\n",
    "                    test_url(url)\n",
    "                else:\n",
    "                    print(\"Please enter a URL\")\n",
    "\n",
    "        # Connect button to function\n",
    "        check_button.on_click(on_button_clicked)\n",
    "\n",
    "        # Display the UI\n",
    "        display(widgets.VBox([widgets.Label(\"Narcotic Website Analyzer\"), url_input, check_button, output]))\n",
    "    except ImportError:\n",
    "        print(\"ipywidgets not available. Use test_url() function directly.\")\n",
    "        \n",
    "# Example usage\n",
    "def main():\n",
    "    # Initialize the classifier\n",
    "    classifier = NarcoticWebsiteClassifier(data_dir=\"data\")\n",
    "    \n",
    "    # Load and preprocess the data\n",
    "    classifier.load_data()\n",
    "    \n",
    "    # Build and train the models\n",
    "    classifier.build_models()\n",
    "    \n",
    "    try:\n",
    "        # Save the model\n",
    "        classifier.save_model()\n",
    "        \n",
    "    \n",
    "        \n",
    "        for url in test_urls:\n",
    "            result = classifier.classify_website(url)\n",
    "            print(f\"\\nResults for {url}:\")\n",
    "            print(f\"  Is narcotic: {result['is_narcotic']}\")\n",
    "            if 'confidence' in result:\n",
    "                print(f\"  Confidence: {result['confidence']:.2f}\")\n",
    "            if 'additional_signals' in result:\n",
    "                print(f\"  Additional signals: {result['additional_signals']}\")\n",
    "            if 'error' in result:\n",
    "                print(f\"  Error: {result['error']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "create_interactive_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
